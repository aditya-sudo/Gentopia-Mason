INFO:root:Creating Config from file: ./gentpool/pool/scholar-agent/agent.yaml
INFO:root:Starting to load configuration from ./gentpool/pool/scholar-agent/agent.yaml
INFO:root:Successfully loaded configuration from ./gentpool/pool/scholar-agent/agent.yaml
INFO:root:--------------------
INFO:root:[green]Welcome to Gentopia!
INFO:root:--------------------
INFO:root:[green]User: 
INFO:root:scholar-agent is thinking...
INFO:root:Done
INFO:root:[blue]scholar-agent: 
INFO:root:{"name":"pdf_reader", "arguments":{"file_path_or_url":"https://arxiv.org/pdf/2402.01135"}}

INFO:root:Calling function: pdf_reader ...
INFO:root:Done
INFO:root:--------------------
INFO:root:Page 1:
A Multi-Agent Conversational Recommender System
Jiabao Fang∗
202215111@mail.sdu.edu.cn
Shandong University
ChinaShen Gao∗
shengao@sdu.edu.cn
Shandong University
ChinaPengjie Ren
jay.ren@outlook.com
Shandong University
China
Xiuying Chen
xiuying.chen@kaust.edu.sa
KAUST
Kingdom of Saudi ArabiaSuzan Verberne
s.verberne@liacs.leidenuniv.nl
Leiden University
NetherlandZhaochun Ren†
z.ren@liacs.leidenuniv.nl
Leiden University
Netherland
ABSTRACT
Due to strong capabilities in conducting fluent, multi-turn con-
versations with users, Large Language Models (LLMs) have the
potential to further improve the performance of Conversational
Recommender System (CRS). Unlike the aimless chit-chat that LLM
excels at, CRS has a clear target. So it is imperative to control the
dialogue flow in the LLM to successfully recommend appropriate
items to the users. Furthermore, user feedback in CRS can assist
the system in better modeling user preferences, which has been
ignored by existing studies. However, simply prompting LLM to
conduct conversational recommendation cannot address the above
two key challenges.
In this paper, we propose Multi-Agent Conversational Recom-
mender System (MACRS) which contains two essential modules.
First, we design a multi-agent act planning framework, which can
control the dialogue flow based on four LLM-based agents. This
cooperative multi-agent framework will generate various candi-
date responses based on different dialogue acts and then choose
the most appropriate response as the system response, which can
help MACRS plan suitable dialogue acts. Second, we propose a
user feedback-aware reflection mechanism which leverages user
feedback to reason errors made in previous turns to adjust the dia-
logue act planning, and higher-level user information from implicit
semantics. We conduct extensive experiments based on user simula-
tor to demonstrate the effectiveness of MACRS in recommendation
and user preferences collection. Experimental results illustrate that
MACRS demonstrates an improvement in user interaction experi-
ence compared to directly using LLMs.
CCS CONCEPTS
•Information systems →Recommender system ;Users and
interactive retrieval .
KEYWORDS
Conversational recommendation, Large language models, Multi-
agent, In-context learning
1 INTRODUCTION
Unlike traditional recommender systems which solely rely on users’
historical behaviors, conversational recommender system (CRS) en-
gages in natural language interactions with the users, allowing the
∗Both authors contributed equally to this research.
†Corresponding author.users to better express their needs [ 12]. Existing CRS frameworks
are generally categorized into two streams: (1) The attribute-based
CRS learns about user preferences by asking questions about the
item attributes and generate responses based on the template. And
the users only need to reply “yes/no” to the questions [ 13,33]. This
approach lacks flexibility as users cannot actively pose questions to
the system, while the system can only reply through human-anno-
tated templates. (2) The generation-based CRS focuses on generat-
ing human-like responses, where users can express their intention
without limitation [ 24,30,34,36]. Most previous generation-based
methods are based on either pre-defined knowledge-graphs or small-
er-scale generative models, which result in poor generalizability
and infeasibility in real-world scenarios.
Due to powerful language generation ability and the extensive
parameterized knowledge embedded in Large Language Models
(LLMs), CRS with an LLM base has recently received more and
more attention [ 4]. Existing CRS with LLMs generally follows two
paradigms: (1) LLM with external RecSys : In these methods, the LLM
is only used for response generation while an external recommender
system is employed to recommend as an individual module [ 3,5,
17]. (2) LLM-only : The LLM in these methods not only facilitates
conversations but also directly generates recommendation results
given user preferences [ 9,29]. However, in recommendation and
dialogue tasks, the LLM integrated with an external RecSys CRS
relies on distinct modules ( a.k.a., recommendation module and
dialogue module). Unfortunately, a majority of current approaches
overlook the exchange of user information between these modules,
consequently having an information gap between them. [ 28,30].
In contrast, LLM-only CRS can share useful information between
the recommendation task and the dialogue task.
LLM-only CRS needs to fulfill both the recommendation task
and the dialogue task, thus it typically contains various dialogue
acts: (1) An asking act that can actively elicit user preferences;
(2) A recommending act that can provide items to users in nat-
ural language form; (3) A chit-chatting act that can attract user
interest and guide the topic of the conversation. Since a conver-
sational recommendation generally has a clearly defined target –
the goal is to recommend a specific item to a user – an engaging
CRS with LLM should be able to control the dialogue flow, meaning
choose the proper dialogue act at each turn. For example, when
user preferences are ambiguous, the CRS can elicit user preferences
through the asking act. And the CRS can utilize the chit-chatting
act to explore various topics and maintain an engaging conversa-
tion. However, previous work shows that the ability of the LLM
1arXiv:2402.01135v1  [cs.IR]  2 Feb 2024

Page 2:
to control the target-directed dialogue flow remains limited [ 10].
For that reason, simply prompting an LLM as a CRS can lead to
unreasonable dialogue flows. For some complex tasks that a single
LLM-based agent may struggle to accomplish, researchers propose
breaking them down into multiple simpler sub-tasks [ 21,22] and
then having multiple LLM-based agents collaborate to jointly com-
plete the complex task. Similarly, CRS is also a type of complex task
involving various intricate dialogue acts. Inspired by these multi-
agent approaches [ 11,21,22], we propose to employ a multi-agent
framework on the CRS. However, designing the architecture of
these agents, defining their interactions, and collaborative methods
to enhance the performance of the CRS remains challenging.
Moreover, the characteristic of CRS is that users and the sys-
tem can engage in multi-turn interactions. During this process,
users can provide their feedback according to the system responses.
Users can actively clarify their preferences in their feedback. And
this feedback also encompasses their satisfaction towards the rec-
ommended items. Therefore, this informative feedback not only
allows the system to obtain more user information but also reflects
the reason why the recommendation failed before. We believe this
feedback can be used to dynamically optimize the system during
the interaction, but this aspect has been overlooked in previous
research.
Recommend
''Zoo to pia"Ask about
the eraChat about
Disney
Dialogue
Flow PlanningResponder
AgentResponder
AgentResponder
Agent
Planner
AgentUserHello! I want to
watch a relaxing
animated film.
Sure! Are you
looking for classic
films or more
recent ones? System
OutputConversational
Recommendation
I would like to find
some recent ones.
MACRS
How about
"Elemental"? It's a
animated film ......
Thanks. But I'm
not really into
romantic storylines.
 User
Feedback
User
Feedback-aware
ReflectionDynamically
AdjustMulti-Agent
Conversational Recommender  System
Figure 1: Example of modeling a Conversational Recommen-
dation System (CRS) using a multi-agent framework. The
right side shows a dialogue between CRS and user, while
the left side depicts the internal framework of MACRS. The
responder agent and planner agent collaboratively gener-
ate appropriate responses, while the reflection mechanism
provides feedback and refined guidance to these agents, opti-
mizing their responses to better satisfy user needs.To address the above issues, we propose Multi-Agent Conver-
sational Recommender System (MACRS), an LLM-only CRS that
can efficiently plan and dynamically refine its dialogue and recom-
mendations. MACRS comprises two essential modules (Figure 1).
The first is multi-agent act planning , a cooperative LLM-based
multi-agent framework which includes three responder agents and
one planner agent. In order to build an engaging dialogue flow,
this framework will plan the dialogue acts for each turn. Each re-
sponder agent in the framework is responsible for performing a
dialogue act and generating a candidate response based on its dia-
logue act. After collecting various candidate responses, the planner
agent will comprehensively consider and select the most appropri-
ate response as the system response. The second module is user
feedback-aware reflection , a dynamic optimization mechanism
based on the LLM. This mechanism leverages the user utterances
as feedback to adjust the responder agents and the planner agent.
The user feedback-aware reflection contains two levels. As user
feedback in each interaction holds significant information about
user preferences, including browsing history and current demand,
information-level reflection becomes an essential aspect to consider.
And the information-level reflection will refine the user feedback to
create higher-level information, referred to as user profiles. These
user profiles will help the responder agents generate more person-
alized responses in subsequent interactions. On the other hand,
when the recommendation fails, the act plan of MACRS needs to be
adjusted in time according to the user feedback. Therefore, reflec-
tion will also operate on the strategy-level, which will deduce the
reasons for the recommend failure and then provide suggestions
and experiences to the agents. Extensive experiments conducted
on Movielens demonstrate the effectiveness of MACRS in recom-
mendation and user preferences collection.
Our contributions of this work are as follows: (i) We propose
MACRS, an LLM-only CRS that focuses on controlling the dialogue
flow for multi-turn interactions. (ii) We design a multi-agent act
planning framework, including three responder agents and one
planner agent. MACRS completes dialogue act planning by guiding
these agents to collaborate. (iii) We design a user feedback-aware
reflection mechanism that leverages the user feedback to dynami-
cally optimize the agent’s response from the information-level and
strategy-level. (iv) Experimental results based on user simulator
illustrate the advantage of MACRS in recommendation accuracy
and efficiency of user preferences collection.
2 RELATED WORK
2.1 LLM-based Autonomous Agents
Autonomous agents are considered to be a promising step towards
artificial general intelligence (AGI), which can complete tasks through
self-directed planning and actions [ 32,38]. Meanwhile, LLMs have
become powerful and fluent chatbots with convincing results in
many topic domains [ 7,19,35]. Therefore, there are many attempts
to use the LLM as central controllers to construct autonomous
agents and thus obtain human-like decision-making capabilities [ 20,
27], which are also called LLM-based agents. For example, Gur et al .
[6]propose an LLM-based agent that can complete user instructions
on real websites by combining canonical web actions in a program
2

Page 3:
space. Schick et al . [25] propose an LLM-based agent that can teach
itself to use external tools via simple APIs.
Recently, a number of researchers have started to explore LLM-
based multi-agent systems, which enable the solution of more com-
plex problems. For example, Hong et al . [11] propose to incorporate
efficient human workflows as a meta programming approach into
LLM-based multi-agent collaboration. In the task of multi-robot
cooperation, Mandi et al . [18] design multiple LLM-based agents
to perform high-level communication and low-level path planning
for different robots. To the best of our knowledge, our study is the
first to propose an LLM-based multi-agent system for conversation
recommendation.
2.2 Conversational Recommender Systems
Conversational Recommender Systems (CRS) aim to recommend
personalized items to users through interactive conversations [ 37].
Conventional CRS can be broadly divided into two categories: The
first category is attribute-based methods, in which the system clar-
ifies user preferences by asking attribute-related questions, and
users respond with “yes/no” to the questions. These methods focus
on when and what to ask before deciding about the item(s) to be
recommended [ 13,23]. For example, [ 13] model conversational rec-
ommendation as a path reasoning problem on a user-item-attribute
graph. However, due to the restricted communication between the
system and the user, the attribute-based methods lack flexibility.
The second category is generation-based methods, in which both
users and the system can communicate using free-form natural
language. Most existing generation-based CRS rely on a knowl-
edge graph for reasoning [ 2,36]. Furthermore, in order to generate
more human-like responses, some prior work uses smaller-scale
generative language models (e.g. DialoGPT based on GPT-2) as
generators [ 28,30]. But due to the limited generalization ability of
knowledge graphs and these smaller language models, previous
generation-based CRS lack practicality in real-world scenarios.
Due to the success of LLMs, many researchers have started incor-
porating it into CRS. With its powerful natural language generation
capabilities and an implicit knowledge base, LLMs have significant
potential in conversational recommendation. Existing LLM-based
CRS can be broadly divided into two categories: (1) Feng et al .
[3], Gao et al . [5], Liu et al . [17] incorporate the LLM with addi-
tional recommendation models. (2) He et al . [9], Wang et al . [29]
use the LLM itself as the CRS. Although directly using LLM for CRS
can share useful knowledge between different tasks, it’s difficult
for the LLM to control the goal-directed dialogue flow. To the best
of our knowledge, our study is the first to propose a controlling
dialogue flow for the CRS completely constructed on LLM.
3 METHODOLOGY
3.1 Overview
Following previous CRS work [ 30], the user initiates the dialogue
with the CRS by a natural language utterance to describe their de-
mand. Then the CRS will respond to the user with an utterance that
may ask the user for more details or chit-chat with the user. After
several turns of dialogue to elicit clear user preferences, the CRS
tries to recommend an item to the user. When the user accepts the
recommended item given by our CRS, we call this dialogue sessionasuccessful sample , and then the user will terminate the dialogue.
In our task setting, we also define a maximum turn threshold .
When the dialogue reaches the maximum turn threshold, but the
CRS still does not provide product suggestions that the user is sat-
isfied with, we have a fallback strategy in which the CRS will give
a recommendation list containing 10 items before terminating the
dialogue. Thus, if the CRS cannot recommend an item that satisfies
the user demand before reaching the maximum turn threshold and
going to the fallback strategy, this dialogue session is referred to as
anunsuccessful sample .
In this paper, to tackle this task, we introduce Multi-Agent Con-
versational Recommender System (MACRS), an LLM-only CRS that
can flexibly control the dialogue flow and dynamically optimize the
system action with user feedback. MACRS consists of two modules.
The first is the multi-agent act planning, a cooperative multi-agent
framework that integrates four LLM-based agents for planning the
multiple dialogue acts, as shown in § 3.2. Secondly, we propose a
user feedback-aware reflection mechanism, which can leverage the
user feedback to adjust the agents in MACRS, as shown in § 3.3.
Figure 2 shows an overview of MACRS.
3.2 Multi-Agent Act Planning
In order to achieve an engaging interaction, a CRS generally can
use various dialogue acts. Therefore, it is essential for the CRS to
effectively plan the dialogue flow by selecting suitable dialogue
acts for each turn. The dialogue flow planning plays a crucial role
in enhancing the efficiency and attractiveness of the CRS. To ac-
complish this complex planning task, we design a multi-agent act
planning framework that contains multiple responder agents and
one planner agent. The responder agents are responsible for gener-
ating various candidate responses based on different dialogue acts.
The planner agent is responsible for devising a dialogue act plan
that can develop an appropriate and fluent dialogue flow.
3.2.1 Responder Agent. An engaging CRS should incorporate a
variety of dialogue acts during multi-turn interactions. Therefore,
we designed three types of responder agents to implement various
dialogue acts including asking, chit-chatting, and recommending.
The asking responder agent 𝜋𝑟
𝑎𝑠𝑘aims at asking user questions to
elicit their preferences. The recommending responder agent 𝜋𝑟𝑟𝑒𝑐
shows an item to the user that it predicts the user may be inter-
ested in. In order to improve the user experience when interacting
with CRS, we cannot repeatedly recommend and ask questions but
need to have timely chit-chats with users about the items using the
chit-chatting responder agent 𝜋𝑟
𝑐ℎ𝑎𝑡. These chit-chats can enhance
the user’s interest in interacting with CRS, and can also better un-
derstand user preferences, like a real salesperson. These responder
agents share the same agent architecture which is composed of
three modules: memory module, profiling module, and action mod-
ule. All these modules are defined through in-context learning with
instructions in the LLM prompt.
The Memory module contains the dialogue history 𝐷ℎ, the user
profile𝑈𝑝(as mentioned in § 3.3.1) and strategy-level suggestions
𝑆(as mentioned in § 3.3.2). The dialogue history 𝐷ℎcomprises the
user utterances and system responses from previous turns. The
user profile 𝑈𝑝contains the user’s current demand and browsing
history. The strategy-level suggestions 𝑆are the guidance for the
3

Page 4:
Memory Module
Profile Module
Action ModuleAsking
Responder  Agent
Candidate Response
based on asking act
Candidate Response
based on recommending act
Candidate Response
based on chit-chatting actMemory Module
Profile Module
Planning ModulePlanner  Agent
Memory Module
Profile Module
Action ModuleRecommending
Responder  Agent
Memory Module
Profile Module
Action ModuleChit-chatting
Responder  Agent
User  Feedback-awar e ReflectionSystem
Response
R sUser
Feedback
U p
UserMulti-Agent Act Planning
Info-level
ReflectStrategy-level
ReflectStrategy-level
ReflectR ask
R rec
R chatFigure 2: The overview of MACRS, which contains two modules: multi-agent act planning and user feedback-aware reflection.
Themulti-agent act planning module generates the response according to the user feedback. The user feedback-aware reflection
module summarizes the high-level user profile and dialogue strategy suggestions which helps the responder agents to optimize
their response.
responder agents in the current turn and are learned from user
feedback in previous turns.
The Profiling module refers to the instructions and indicates
the role of the responder agent. Obviously, responder agents for
different dialogue acts need to complete different task objectives,
we separately design the instructions for each of the agents:
(1) Instruction 𝐼𝑎𝑠𝑘for asking responder agent 𝜋𝑟
𝑎𝑠𝑘:
You should elicit user preferences by asking questions.
(2) Instruction 𝐼𝑐ℎ𝑎𝑡 for chit-chatting agent 𝜋𝑟
𝑐ℎ𝑎𝑡:
You should chit-chat with the user to learn about their pref-
erences. You can express your admiration for certain item
elements to guide the conversation towards them, thereby
gaining insights into the user preferences regarding those
elements.
(3) Instruction 𝐼𝑟𝑒𝑐for recommending agent 𝜋𝑟𝑟𝑒𝑐:
You should recommend an item to user and generate an
engaging description about the item.
The Action module generates three candidate responses 𝑅𝑎𝑠𝑘,
𝑅𝑐ℎ𝑎𝑡 and𝑅𝑟𝑒𝑐based on the memory and profile:𝑅∗=𝜋∗
𝑟(𝐼∗,𝐷ℎ,𝑈𝑝,𝑆∗), (1)
where∗indicates“ask”, “rec” or “chat” respectively.
3.2.2 Planner Agent. In order to control the dialogue flow, it is
important to select the most suitable response from the candidate
responses𝑅𝑎𝑠𝑘,𝑅𝑐ℎ𝑎𝑡,and𝑅𝑟𝑒𝑐as the system response 𝑅𝑠. To tackle
this task, the planner agent 𝜋𝑝utilizes multi-step reasoning to select
responses across multiple dimensions, such as informativeness and
engagement. For example, when the available user preferences are
insufficient, the planner agent reasons which candidate response
can potentially yield a higher user information gain. Then the
planner agent will select the candidate response 𝑅𝑎𝑠𝑘of the ask
responder agent as the final response to the user to gather more user
preferences. To achieve this goal, we propose a novel architecture
of planner agent 𝜋𝑝which is composed of a memory module, a
profiling module, and a planning module:
The Memory module contains the dialogue history 𝐷ℎ, the
dialogue act history 𝐴ℎ, and corrective experiences 𝐸(introduced in
§ 3.3.2). The dialogue act history 𝐴ℎrecords the dialogue act selected
by the planner agent in previous dialogue turns. The corrective
experiences 𝐸are learned from user feedback in past turns.
The Profiling module indicates both its role and the back-
ground knowledge. Instruction 𝐼𝑝𝑙𝑎𝑛 for the planner agent 𝜋𝑝:
4

Page 5:
Choose one of the candidate responses based on three
different dialogue acts. These three dialogue acts are: rec-
ommending, asking, and chit-chatting.
The Planning module performs multi-step reasoning based
on its memory and its profile to make an appropriate dialogue act
selection.
•Step 1: Review past acts in the dialogue act history 𝐴ℎand
avoid the repetition of the same act across multiple turns.
•Step 2: Determine whether the available user preferences are
sufficient based on the dialogue history 𝐷ℎ. If so, the recommending
act is a suitable choice.
•Step 3: If the available user preferences are insufficient, deter-
mine which candidate response is more appropriate or can yield
greater user information gain.
After multi-step reasoning, the planner agent 𝜋𝑝will output a
selection, which in turn determines the final system response 𝑅𝑠:
𝑅𝑠=𝜋𝑝(𝐼𝑝𝑙𝑎𝑛,𝑅𝑎𝑠𝑘,𝑅𝑟𝑒𝑐,𝑅𝑐ℎ𝑎𝑡,𝐷ℎ,𝐴ℎ,𝐸). (2)
3.3 User Feedback-aware Reflection
User
FeedbackInformation-level
Reflection
Strategy-level
ReflectionUser Profile U p
Strategy-level
Suggestions S
Corrective
Experience EMemory Module of
Planner AgentMemory Module of
Asking
Responder Agent
Memory Module of
Recommending
Responder Agent
Memory Module of
Chit-chatting
Responder Agent
Figure 3: Overview of user feedback-aware reflection mecha-
nism.
During the interactions with the CRS, the user provides the
feedback utterance 𝑈𝑓to the system response. On the one hand,
users will actively express their preferences in the feedback by pro-
viding more detailed descriptions or responding to questions. On
the other hand, users will also implicitly convey their satisfaction
with the recommendations and occasionally clarify the reasons
behind their satisfaction. In order to leverage this valuable feed-
back𝑈𝑓to dynamically optimize our agents, we propose a user
feedback-aware reflection mechanism that operates at two levels.
Figure 3 provides a illustration for the proposed mechanism. The
first information-level reflection 𝜋info_reflect infers user preferences
from user feedback and summarizes them as user profiles 𝑈𝑝. The
second strategy-level reflection 𝜋stra_reflect adjusts the dialogue act
plan by generating suggestions 𝑆and corrective experiences 𝐸
based on the user’s clarifications to the recommendations. Again,
all strategies are implemented through in-context learning.
3.3.1 Information-level Reflection. We propose the information-
level reflection mechanism 𝜋info_reflect which summarizes user feed-
back to higher-level information, referred as to user profiles 𝑈𝑝. Theuser profiles 𝑈𝑝contain current user demand and browsing history:
(1) The current user demand is stored in a dictionary format, com-
prising multiple key-value pairs. Each key-value pair represents an
item attribute that the user is interested in. And these key-value
pairs are extracted from the user feedback by using this mechanism
𝜋info_reflect . (2) The browsing history includes multiple items men-
tioned by the user, which are extracted from the user feedback 𝑈𝑓.
It helps the responder agents effectively capture user preferences
and also reminds them to avoid generating duplicate content.
In dialogue turn 𝑡, the information-level reflection mechanism
𝜋info_reflect generates the user profile 𝑈𝑡𝑝based on the user profile
𝑈𝑡−1𝑝from the previous turn 𝑡−1, system response 𝑅𝑡−1𝑠and user
feedback𝑈𝑡−1
𝑓:
𝑈𝑡
𝑝=𝜋info_reflect(𝑈𝑡−1
𝑝,𝑅𝑡−1
𝑠,𝑈𝑡−1
𝑓), (3)
where the information-level reflection mechanism 𝜋info_reflect is
implemented by prompting an LLM-based agent using the following
instructions:
Please infer user preferences based on the conversation.
And combine them with the past preferences to summarize
a more complete user preferences.
After information-level reflection, the user profiles 𝑈𝑝will be
transmitted to the memory module of each responder agent.
3.3.2 Strategy-level Reflection. User feedback implicitly indicates
their satisfaction with the recommendations. When a user is dissat-
isfied, it indicates that MACRS has issues in controlling the dialogue
flow in the past few turns. So we propose the strategy-level reflec-
tion mechanism 𝜋stra_reflect which mainly focuses on dynamically
adjusting the dialogue act plan when recommendation fails. This
mechanism𝜋stra_reflect is implemented by prompting an LLM-based
agent.
Our proposed strategy-level reflection mechanism 𝜋stra_reflect
will first collect multi-turn trajectory 𝑀, which consists of three
parts: user profile 𝑈𝑝(as mentioned in § 3.3.1), system response 𝑅𝑠
and user feedback 𝑈𝑓. Then the trajectory 𝑀from turn𝑖to turn𝑡
can be represented as:
𝑀𝑖
𝑡=
(𝑈𝑖
𝑝,𝑅𝑖
𝑠,𝑈𝑖
𝑓),...,(𝑈𝑡
𝑝,𝑅𝑡
𝑠,𝑈𝑡
𝑓)
. (4)
When the system provide a failed recommendation in turn 𝑡, this
mechanism 𝜋stra_reflect reflects on the trajectory from turn 𝑖to
turn𝑡, where turn 𝑖refers to the turn immediately following the
last recommendation failure. If turn 𝑡is the first recommendation
failure, then 𝑖=1. To reflect the mistakes the system has made
in the trajectory 𝑀𝑖
𝑡, strategy-level reflection generates an error
summary𝐸𝑠:
𝐸𝑠=𝜋stra_reflect(𝑀𝑖
𝑡), (5)
where the error summary 𝐸𝑠is a descriptive natural language rep-
resentation that encompasses errors made by both the responder
agents and the planner agent. The instruction of error summary 𝐸𝑠
generation is as follow:
5

Page 6:
Based on your past action trajectory, your goal is to write a
few sentences to explain why your recommendation failed
as indicated by the user utterance.
To avoid repeating the same mistake, the strategy-level reflection
mechanism 𝜋stra_reflect adjusts the agents by generating an error
summary𝐸𝑠. There are two main parts in the error summary 𝐸𝑠:
the strategy-level suggestions 𝑆and the corrective experiences 𝐸.
The strategy-level suggestions 𝑆indicate the dialogue strategies
that each agent needs to complete in the next dialogue turn and
will be used in the memory module of each responder agent ( e.g.,
ask, rec and chat). For example, a user rejects the recommendation
and responds with “I want to watch more classic films”, which
is vague. The mechanism can suggest the asking responder agent
𝜋𝑟
𝑎𝑠𝑘to elicit specific preferences regarding the release time of films.
Secondly, the strategy-level reflection summarizes the suggestions
𝑆into corrective experiences 𝐸and stores them in the memory
module of the planner agent. These experiences will influence its
subsequent planning process. The instruction of strategy-level sug-
gestions𝑆and corrective experiences 𝐸is as follow:
You need to generate several suggestions to “Recommend-
ing Agent”, “Asking Agent” and “Chit-chatting Agent”.
Then you should report the suggestions to the “Planning
Agent” as experiences.
4 EXPERIMENTAL SETUP
We evaluate the effectiveness of MACRS in both user preferences
collection and recommendation.
4.1 User Simulator
To evaluate the performance of MACRS in multi-turn interactions,
we develop a LLM-based user simulator inspired by [ 29]. We con-
struct the user simulator based on ChatGPT and define the prefer-
ence of the user simulator using the browsing history and target
item information. In prior work [ 29], the target item is directly
given to the user simulator, which leads to the simulator describing
the target item in excessive detail. We argue that this setting does
not align with real-life scenarios, in which users usually have fuzzy
preferences [ 15]. In order to create a more realistic and natural
user simulator, we use ChatGPT to summarize the information of
the target item into multiple keywords, referred to as target item
profiles. Then we replace the name of target items with the target
item profiles and request the user simulator to avoid overly direct
descriptions. The instruction prompt of the user simulator is:
You are a user chatting with an assistant for movie recom-
mendation in turn. Your browsing history can reflect your
past preferences. And you will seek recommendations from
the assistant based on the [target movie information] .where the [target movie information] represents the target item
profiles.
4.2 Implementation Details
In our experiments, all MACRS-C variants and the ChatGPT base-
line use the gpt-3.5-turbo-0613 version, and the MACRS-L variant
and Llama2 baseline use the Llama-2-70b-chat-hf version which
is open-sourced. We use the temperature 0.0 in our experiments.
And we update the memory modules of agents in MACRS after
each turn, meaning that only the suggestions and experiences from
the previous turn are retained. For a fair comparison, we maintain
consistency in the item summary and user utterance of the first
turn across all experiments. We set the maximum threshold of turns
as 5. And when the CRS is still unsuccessful in the 5th turn, it will
directly generate a recommendation list in the 6th turn.
4.3 Evaluation Metrics
To quantitatively measure the performance of MACRS, we design
three automatic metrics:
(1)Success Rate : To evaluate the overall efficiency of CRSs, we
calculate the rate of successful samples (definition can be found at
§ 3.1) over all the test sets:
Success_Rate =𝑁𝑠𝑢
𝑁, (6)
where𝑁is the number of test set samples, and 𝑁𝑠𝑢is the number
of successful samples.
(2)Hit Ratio@K : Since our method has a fallback strategy
of recommending an item list at the end of the conversation, we
also include this part of the recommendation results in the final
recommendation effect evaluation. This allows us to evaluate the
efficiency of information gathering. We calculate the number of
top-𝐾hits in recommendation lists, where 𝐾=5,10. We also add
the number of successful samples in the final calculation:
Hit_Ratio =Í𝑁𝑢𝑛
𝑖=1ℎ𝑖𝑡(𝑖)+𝑁𝑠𝑢
𝑁, (7)
where𝑁𝑢𝑛is the number of unsuccessful samples (definition can
be found at § 3.1). And when the target item appears in the recom-
mendation list, ℎ𝑖𝑡(𝑖)=1. Otherwise, ℎ𝑖𝑡(𝑖)=0.
(3)Average Turns : To evaluate the recommendation efficiency
of the CRS, we calculate the average number of turns in the con-
versation:
Average_Turns =Í𝑁
𝑖=1𝑁𝑇𝑖
𝑁, (8)
where𝑁𝑇𝑖is the number of turns in sample 𝑖.
4.4 Dataset
We conduct our experiments on the MovieLens [ 8] which is a bench-
mark dataset for movie recommendation. It contains 162,541 users,
62,423 movies and 25,000,095 ratings. To construct the user simu-
lator, we truncate the interaction sequence to 20 items, utilizing
them to create the browsing history. The 21st item is then used as
the prediction target item. Then we filter the browsing history and
only retain the items that belong to the same category as the target
6

Page 7:
Table 1: Conversational recommendation performance comparison. ‡indicates significant improvement over ChatGPT with
𝑝≤0.01according to a Student’s t test.
Method Success Rate (↑) Average Turns ( ↓) Hit Ratio@5 (↑) Hit Ratio@10 (↑)
Traditional CRS Baselines
KBRD [2] 0.00 6.0 0.00 0.01
BARCOR [28] 0.03 5.9 0.03 0.07
LLM-based CRS Baselines
Llama2 0.42 4.34 0.47 0.51
ChatGPT 0.39 4.24 0.44 0.51
Our Proposed Method
MACRS-L 0.48 4.49 0.55 0.60
MACRS-C 0.61‡4.19†0.77‡0.80‡
MACRS-C w/o IR 0.53 4.42 0.72 0.77
MACRS-C w/o SR 0.52 4.42 0.67 0.73
MACRS-C w/o SR+IR 0.51 4.40 0.64 0.66
item. We randomly select 100 samples and ensured that the length
of their browsing history is not less than 5.
4.5 Comparison Methods
To evaluate the effectiveness of MACRS, we compare it to four
strong CRS baseline methods:
(1)KBRD [2] contains a recommendation module and a Transformer-
based dialogue module. It leverages an external knowledge graph
to enhance the semantics of entities mentioned in the dialogue
history.
(2)BARCOR [28] uses the bidirectional encoder of BART [ 14] as the
recommendation module and its auto-regressive decoder as the
dialogue module.
(3)ChatGPT1is a closed-source LLM from OpenAI. We use the
version gpt-3.5-turbo-0613. We construct a powerful single-agent
CRS based on it using the same prompt as our model.
(4)Llama22is an open-source LLM that has been trained using both
supervised fine-tuning and reinforcement learning with human
feedback [ 26]. We use the Llama2 model with 70 billion parameters.
We construct it as a single-agent method to plan the dialogue flow
and compare it with our multi-agent framework.
We propose two variants of MACRS: MACRS-C andMACRS-L ,
which use the ChatGPT andLlama2 as the backbone respectively.
To demonstrate the effectiveness of each module in our proposed
MACRS framework, we employ several ablation models in our
experiments:
(1)MACRS w/o SR : We remove the strategy reflection (SR) module
in MACRS, which can verify the effectiveness of the reflection on
the dialogue strategy.
(2)MACRS w/o IR : We remove the information reflection (IR) mod-
ule in our user feedback-aware reflection which generates the user
profile, which verifies the effectiveness of high-level user preference
information.
1https://chat.openai.com/
2https://huggingface.co/meta-llama/Llama-2-70b-chat-hf(3)MACRS w/o SR+IR : We remove both of the strategy reflection
andinformation reflection (SR+IR) to verify the effectiveness of
the reflection mechanism in MACRS.
All the ablation studies are conducted on MACRS-C .
5 EXPERIMENTAL RESULTS
5.1 Main Results
Table 1 shows the performance of our proposed MACRS and base-
lines in terms of four metrics. We see that the LLM-only CRSs ( e.g.,
ChatGPT andLlama2 ) demonstrate superior performance compared
to traditional CRS methods ( e.g., BARCOR andKBRD ). On the one
hand, this is because BARCOR andKBRD rely on knowledge graphs
and face limitations in successfully recommending items that are
not present in the knowledge graph. On the other hand, they rely
on smaller-scale generative models that exhibit weaker reasoning
and natural language understanding capabilities than LLMs.
We also find that MACRS-C outperforms its backbone LLM
ChatGPT in terms of all metrics. This suggests that with the help of
our proposed MACRS framework, the LLM-only CRS can improve
in both user preference gathering and recommendation accuracy.
Although the MACRS-L outperforms the Llama2 in terms of all
the recommendation accuracy metrics ( e.g.,Success Rate and Hit
Ratio), we observe that MACRS-L has a slightly higher average
number of turns compared to the Llama2 . We suspect that this is
because MACRS method recommends more cautiously, which gives
it an advantage in both success rate and recommendation accuracy.
In contrast, the single-agent methods ( e.g.,ChatGPT andLlama2 )
tend to directly and frequently recommend items to the user, and
this behavior will lead to higher efficiency. However, it is obvious
that such frequent recommendations not only risk user dissatisfac-
tion but are also inefficient for capturing user preferences. A more
detailed analysis can be found in § 5.3.1.
Moreover, while Llama2 exhibits enhanced performance com-
pared to ChatGPT as a single agent, MACRS-L lags behind MACRS-C
across all metrics. We speculate the reason for this phenomenon
is the Llama2 ’s weaker ability to follow the complex instructions
7

Page 8:
Figure 4: The ratio of different dialogue acts chosen in each turn of dialogue. The diversity in dialogue act selection of our
proposed MACRS is remarkably enhanced compared to the corresponding backbone model, and it enhances the user experience.
than ChatGPT , which results in suboptimal outcomes within our
proposed multi-agent framework.
1 2 3 4 5
Turn102030405060Cumulative number of successful samples
ChatGPT
MACRS-C
Llama2
MACRS-L
Figure 5: Cumulative number of successful samples for each
dialogue turn.
5.2 Ablation Study
To evaluate the effectiveness of each module in MACRS, we conduct
ablation studies with three variant models: (1) only using the multi-
agent act planning without both levels of reflection ( a.k.a., MACRS
w/o SR+IR); (2) only using the information-level user feedback-
aware reflection ( a.k.a., MACRS w/o SR); (3) only using the strategy-
level user feedback-aware reflection ( a.k.a., MACRS w/o IR). The
performance of ablation models is shown in Table 1, and we can
find that all of the ablation models perform less promising than
the best model MACRS-C, which demonstrates the preeminence
of each module in MACRS. We can also find that the MACRS w/o
SR performs worse than the MACRS w/o IR, which demonstrates
that strategy-level suggestion and corrective experience are more
important of the CRS.
5.3 Discussion
5.3.1 Analysis of different dialogue acts ratio. In order to compare
the differences in dialogue act planning between MACRS and single-
agent CRS, we conduct a statistical analysis comparing the dialogue
1 2 3 4 5
Popularity level4.55.05.56.0Turn Average
 ChatGPT
MACRS-C
1 2 3 4 5
Popularity level0.02.55.07.510.012.5Success Rate
ChatGPT
MACRS-C
1 2 3 4 5
Popularity level051015Hit Ratio@5
ChatGPT
MACRS-C
1 2 3 4 5
Popularity level051015Hit Ratio@10
ChatGPT
MACRS-CFigure 6: Performance of different models with respect to
the popularity level of item.
act selection in each turn. As shown in Figure 4, our method has bet-
ter diversity in choosing dialogue acts than other baseline models.
Other baseline methods ( e.g.,ChatGPT ) tend to directly recommend
or ask the user’s preference in each round of dialogue. This will
lead to a poor user experience with the CRS, as users are often
impatient with being constantly recommended boring items. In
addition, Figure 4 shows that our method tends to ask the user’s
preference in the first round of dialogue, make a trial recommen-
dation in the second round, and then adjust the dialogue act based
on the previous interaction in the subsequent turns. MACRS also
inserts a small amount of chit-chat acts at appropriate times dur-
ing the dialogue to enhance the user experience and increase the
appeal of the CRS to users. We expect this conversational behavior
of the CRS to be more natural and human-like and will therefore
be preferred by users.
We also conduct a statistical analysis on the number of cumula-
tive successful samples at each turn 𝑇, where𝑇={1,2,3,4,5}. As
shown in Figure 5, since MACRS tends to ask about user preferences
in the initial turns, while ChatGPT tends to directly make recom-
mendations, ChatGPT has more accumulative successful examples
when𝑇={1,2}. However, as ChatGPT lacks explicit modeling
8

Page 9:
Table 2: The influence of strategy-level suggestions.
Response
w/ Suggestions WinsTieResponse
w/o Suggestions Wins
45.4% 33 .0% 21 .6%
of user preferences, the cumulative count of successfully recom-
mended samples gradually decreases in later interaction turns. This
experiment also demonstrates the importance of actively obtaining
user preference information.
5.3.2 Analysis of the influence of item popularity. Popularity bias,
a well-known phenomenon in recommendation systems [ 1,16,31],
can lead to reduced user satisfaction when the target item is unpop-
ular. We conducted a statistical analysis to examine the performance
differences of MACRS under varying levels of target item popularity
and we compared it with the LLM-based CRS baseline ChatGPT . In
the experiments, we quantify the popularity of an item based on
the number of interactions and categorize it into five levels: top
10%, top 10%-20%, top 20%-30%, top 30%-50%, and bottom 50%. For
each level, we randomly selected 20 samples.
The results are shown in Figure 6. The experimental results indi-
cate that MACRS outperforms the baseline in all popularity levels
of items. Moreover, MACRS demonstrates remarkable recommenda-
tion accuracy when dealing with challenging samples characterized
by lower item popularity. This can be attributed to the effective
dialogue act planning in MACRS, which enables it to gather more
comprehensive user preference information.
On the contrary, the single-agent method ChatGPT tends to make
more frequent recommending acts, which can help improve the
overall efficiency and reduce the number of average number of
dialogue turns on the test set. However, this approach has a negative
effect on low-popularity items, which can make it difficult for these
CRS methods to recommend low-popularity items to users, as they
lack a detailed understanding of user preferences.
5.3.3 The influence of strategy-level suggestions. In order to evalu-
ate the strategy-level suggestions generated by the strategy-level
reflection mechanism, we compared the responses with sugges-
tions and the responses without suggestions. We use ChatGPT3as
the evaluator. We instruct the evaluator to compare the responses
according to multiple criteria including (1) recommendation accu-
racy, (2) user information gain, (3) conciseness, and (4) engagement.
Then the evaluator selects the superior response based on these
criteria. The result shown in Table 2 indicates that the strategy-level
suggestions can enhance the quality of the responses.
5.4 Case Study
In Figure 7, we show an example of the dialogue between the user
and MACRS and between the user and the baseline method. The text
in green indicates whether the user accepts the CRS recommended
item, the text in red represents dialogue acts that are beneficial for
recommendation. From this case, we can find that the ChatGPT fails
to recommend the proper item to the user since it does not further
explore user preferences in the 2nd turn. When the user chats with
MACRS, MACRS accurately identifies user implicit preferences
3https://chat.openai.com/for directors when the user replies with “the director of it did not
meet my expectations”. It is essential for the CRS to detect implicit
preferences from user feedback and further explore them to uncover
more explicit preferences. This observation demonstrates that our
proposed user feedback-aware reflection module can help the CRS
adjust the dialogue act plan based on user feedback.
Item Information
Target item : "Eraser (1996)"
Target item information : {genre: action, actor: Arnold
Schwarzenegger , director: Chuck Russell, feature: witness
protection}
Dialogue between ChatGPT  and the user
User : Hello, I'm in the mood for an action movie. I really enjoy
watching Arnold Schwarzenegger films. Can you recommend
something for me? 
ChatGPT : How about watching "T erminator 2: Judgment Day"? It's
an iconic action film starring Arnold Schwarzenegger . 
User : I haven't seen "T erminator 2: Judgment Day". But the
director of it did not meet my expectations.
ChatGPT : If you haven't seen "T erminator 2: Judgment Day", how
about watching "Predator"? It's another action film ... ... 
User : I've seen ``Predator'', but the director of it did not meet my
expectations.
FAIL
Dialogue between MACRS and the user
User : Hello, I'm in the mood for an action movie. I really enjoy
watching Arnold Schwarzenegger films. Can you recommend
something for me?
MACRS : Sure. Do you prefer more  recent action movies or older
ones?
User : I prefer older action movies.
MACRS : How about watching ``T erminator 2: Judgment Day''? It's
an older  action film starring Arnold Schwarzenegger .
User : I haven't seen ``T erminator 2: Judgment Day''. But the
director of it did not meet my expectations.
MACRS : What type of director do you prefer  for action movies? 
User : I prefer Chuck Russell for action movies. 
MACRS : How about watching ``Eraser''? It's an older  action film
starring Arnold Schwarzenegger and directed by Chuck Russell .
SUCCESS
Figure 7: Case study for a conversational recommendation.
6 CONCLUSION
In this paper, we introduced Multi-Agent Conversational Recom-
mender System (MACRS), an LLM-only multi-agent Conversational
Recommender System (CRS) that leverages several different agents
to tackle the dialogue flow planning and integrate the user feedback
to optimize the dialogue planning dynamically. We first propose a
novel multi-agent dialogue planning module to separately model
each dialogue act using LLM-based responder agents and sched-
ule these responder agents according to the dialogue strategy and
user feedback by using the planner agent. Then we propose a user
feedback-aware reflection module that conducts the reflection on
both information-level and strategy-level to produce high-level
9

Page 10:
user preference descriptions and dialogue strategy suggestions for
the subsequent dialogue. These two modules can collaborate to
optimize the dialogue flow and increase the user experience and
recommendation accuracy. Experiments conducted on a benchmark
CRS dataset demonstrate that our proposed MACRS can boost the
performance of recommendations significantly and have a better
and more engaging user experience than with the existing CRS
methods.
REFERENCES
[1]Himan Abdollahpouri, Masoud Mansoury, Robin Burke, and Bamshad Mobasher.
2020. The connection between popularity bias, calibration, and fairness in recom-
mendation. In Proceedings of the 14th ACM Conference on Recommender Systems .
726–731.
[2]Qibin Chen, Junyang Lin, Yichang Zhang, Ming Ding, Yukuo Cen, Hongxia Yang,
and Jie Tang. 2019. Towards knowledge-based recommender dialog system. arXiv
preprint arXiv:1908.05391 (2019).
[3]Yue Feng, Shuchang Liu, Zhenghai Xue, Qingpeng Cai, Lantao Hu, Peng Jiang,
Kun Gai, and Fei Sun. 2023. A Large Language Model Enhanced Conversational
Recommender System. arXiv preprint arXiv:2308.06212 (2023).
[4]Luke Friedman, Sameer Ahuja, David Allen, Terry Tan, Hakim Sidahmed,
Changbo Long, Jun Xie, Gabriel Schubiner, Ajay Patel, Harsh Lara, et al .2023.
Leveraging Large Language Models in Conversational Recommender Systems.
arXiv preprint arXiv:2305.07961 (2023).
[5]Yunfan Gao, Tao Sheng, Youlin Xiang, Yun Xiong, Haofen Wang, and Jiawei
Zhang. 2023. Chat-rec: Towards interactive and explainable llms-augmented
recommender system. arXiv preprint arXiv:2303.14524 (2023).
[6]Izzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa Safdari, Yutaka Matsuo, Dou-
glas Eck, and Aleksandra Faust. 2023. A real-world webagent with planning, long
context understanding, and program synthesis. arXiv preprint arXiv:2307.12856
(2023).
[7]Muhammad Usman Hadi, Rizwan Qureshi, Abbas Shah, Muhammad Irfan, Anas
Zafar, Muhammad Bilal Shaikh, Naveed Akhtar, Jia Wu, Seyedali Mirjalili, et al .
2023. Large language models: a comprehensive survey of its applications, chal-
lenges, limitations, and future prospects. Authorea Preprints (2023).
[8]F. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History
and Context. ACM Trans. Interact. Intell. Syst. 5, 4, Article 19 (dec 2015), 19 pages.
https://doi.org/10.1145/2827872
[9]Zhankui He, Zhouhang Xie, Rahul Jha, Harald Steck, Dawen Liang, Yesu Feng,
Bodhisattwa Prasad Majumder, Nathan Kallus, and Julian McAuley. 2023. Large
language models as zero-shot conversational recommenders. In Proceedings of the
32nd ACM international conference on information and knowledge management .
720–730.
[10] Joey Hong, Sergey Levine, and Anca Dragan. 2023. Zero-Shot Goal-Directed
Dialogue via RL on Imagined Conversations. arXiv preprint arXiv:2311.05584
(2023).
[11] Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao
Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, et al .2023.
Metagpt: Meta programming for multi-agent collaborative framework. arXiv
preprint arXiv:2308.00352 (2023).
[12] Dietmar Jannach, Ahtsham Manzoor, Wanling Cai, and Li Chen. 2021. A survey
on conversational recommender systems. ACM Computing Surveys (CSUR) 54, 5
(2021), 1–36.
[13] Wenqiang Lei, Gangyi Zhang, Xiangnan He, Yisong Miao, Xiang Wang, Liang
Chen, and Tat-Seng Chua. 2020. Interactive path reasoning on graph for conver-
sational recommendation. In Proceedings of the 26th ACM SIGKDD international
conference on knowledge discovery & data mining . 2073–2083.
[14] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman
Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART:
Denoising Sequence-to-Sequence Pre-training for Natural Language Generation,
Translation, and Comprehension. In Proceedings of the 58th Annual Meeting of
the Association for Computational Linguistics . 7871–7880.
[15] Belinda Z Li, Alex Tamkin, Noah Goodman, and Jacob Andreas. 2023. Eliciting
human preferences with language models. arXiv preprint arXiv:2310.11589 (2023).
[16] Allen Lin, Jianling Wang, Ziwei Zhu, and James Caverlee. 2022. Quantifying and
mitigating popularity bias in conversational recommender systems. In Proceedings
of the 31st ACM International Conference on Information & Knowledge Management .
1238–1247.
[17] Yuanxing Liu, Wei-Nan Zhang, Yifan Chen, Yuchi Zhang, Haopeng Bai, Fan Feng,
Hengbin Cui, Yongbin Li, and Wanxiang Che. 2023. Conversational Recommender
System and Large Language Model Are Made for Each Other in E-commerce
Pre-sales Dialogue. arXiv preprint arXiv:2310.14626 (2023).
[18] Zhao Mandi, Shreeya Jain, and Shuran Song. 2023. Roco: Dialectic multi-robot
collaboration with large language models. arXiv preprint arXiv:2307.04738 (2023).[19] Bonan Min, Hayley Ross, Elior Sulem, Amir Pouran Ben Veyseh, Thien Huu
Nguyen, Oscar Sainz, Eneko Agirre, Ilana Heintz, and Dan Roth. 2023. Recent
advances in natural language processing via large pre-trained language models:
A survey. Comput. Surveys 56, 2 (2023), 1–40.
[20] Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy
Liang, and Michael S Bernstein. 2023. Generative agents: Interactive simulacra
of human behavior. In Proceedings of the 36th Annual ACM Symposium on User
Interface Software and Technology . 1–22.
[21] Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan
Liu, and Maosong Sun. 2023. Communicative agents for software development.
arXiv preprint arXiv:2307.07924 (2023).
[22] Shuofei Qiao, Ningyu Zhang, Runnan Fang, Yujie Luo, Wangchunshu Zhou,
Yuchen Eleanor Jiang, Chengfei Lv, and Huajun Chen. 2024. AUTOACT:
Automatic Agent Learning from Scratch via Self-Planning. arXiv preprint
arXiv:2401.05268 (2024).
[23] Xuhui Ren, Hongzhi Yin, Tong Chen, Hao Wang, Zi Huang, and Kai Zheng.
2021. Learning to ask appropriate questions in conversational recommendation.
InProceedings of the 44th international ACM SIGIR conference on research and
development in information retrieval . 808–817.
[24] Zhaochun Ren, Zhi Tian, Dongdong Li, Pengjie Ren, Liu Yang, Xin Xin, Huasheng
Liang, Maarten de Rijke, and Zhumin Chen. 2022. Variational Reasoning about
User Preferences for Conversational Recommendation. In Proceedings of the 45th
International ACM SIGIR Conference on Research and Development in Information
Retrieval . 165–175.
[25] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli,
Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Lan-
guage models can teach themselves to use tools. arXiv preprint arXiv:2302.04761
(2023).
[26] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yas-
mine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhos-
ale, et al .2023. Llama 2: Open foundation and fine-tuned chat models. arXiv
preprint arXiv:2307.09288 (2023).
[27] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang,
Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. 2023. A survey on large
language model based autonomous agents. arXiv preprint arXiv:2308.11432 (2023).
[28] Ting-Chun Wang, Shang-Yu Su, and Yun-Nung Chen. 2022. BARCOR: Towards A
Unified Framework for Conversational Recommendation Systems. arXiv preprint
arXiv:2203.14257 (2022).
[29] Xiaolei Wang, Xinyu Tang, Wayne Xin Zhao, Jingyuan Wang, and Ji-Rong Wen.
2023. Rethinking the Evaluation for Conversational Recommendation in the Era
of Large Language Models. arXiv preprint arXiv:2305.13112 (2023).
[30] Xiaolei Wang, Kun Zhou, Ji-Rong Wen, and Wayne Xin Zhao. 2022. Towards
unified conversational recommender systems via knowledge-enhanced prompt
learning. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Dis-
covery and Data Mining . 1929–1937.
[31] Tianxin Wei, Fuli Feng, Jiawei Chen, Ziwei Wu, Jinfeng Yi, and Xiangnan He.
2021. Model-agnostic counterfactual reasoning for eliminating popularity bias
in recommender system. In Proceedings of the 27th ACM SIGKDD Conference on
Knowledge Discovery & Data Mining . 1791–1800.
[32] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming
Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, et al .2023. The rise and potential
of large language model based agents: A survey. arXiv preprint arXiv:2309.07864
(2023).
[33] Kerui Xu, Jingxuan Yang, Jun Xu, Sheng Gao, Jun Guo, and Ji-Rong Wen. 2021.
Adapting user preference to online feedback in multi-round conversational rec-
ommendation. In Proceedings of the 14th ACM international conference on web
search and data mining . 364–372.
[34] Xiaoyu Zhang, Xin Xin, Dongdong Li, Wenxuan Liu, Pengjie Ren, Zhumin Chen,
Jun Ma, and Zhaochun Ren. 2023. Variational Reasoning over Incomplete Knowl-
edge Graphs for Conversational Recommendation. In Proceedings of the Sixteenth
ACM International Conference on Web Search and Data Mining . 231–239.
[35] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou,
Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al .2023. A survey
of large language models. arXiv preprint arXiv:2303.18223 (2023).
[36] Kun Zhou, Wayne Xin Zhao, Shuqing Bian, Yuanhang Zhou, Ji-Rong Wen, and
Jingsong Yu. 2020. Improving conversational recommender systems via knowl-
edge graph based semantic fusion. In Proceedings of the 26th ACM SIGKDD
international conference on knowledge discovery & data mining . 1006–1014.
[37] Kun Zhou, Yuanhang Zhou, Wayne Xin Zhao, Xiaoke Wang, and Ji-Rong Wen.
2020. Towards topic-guided conversational recommender system. arXiv preprint
arXiv:2010.04125 (2020).
[38] Wangchunshu Zhou, Yuchen Eleanor Jiang, Long Li, Jialong Wu, Tiannan Wang,
Shi Qiu, Jintian Zhang, Jing Chen, Ruipu Wu, Shuai Wang, et al .2023. Agents:
An open-source framework for autonomous language agents. arXiv preprint
arXiv:2309.07870 (2023).
10

INFO:root:--------------------
INFO:root:scholar-agent is thinking...
INFO:root:Done
INFO:root:[blue]scholar-agent: 
INFO:root:The paper titled "A Multi-Agent Conversational Recommender System" presents a novel approach to enhancing Conversational Recommender Systems (CRS) using Large Language Models (LLMs). The authors propose a system called Multi-Agent Conversational Recommender System (MACRS), which addresses key challenges in controlling dialogue flow and incorporating user feedback to improve recommendations.

### Key Contributions:
1. **Multi-Agent Framework**: MACRS employs a multi-agent act planning framework consisting of four LLM-based agents. This framework generates various candidate responses based on different dialogue acts (asking, recommending, chit-chatting) and selects the most appropriate response to guide the conversation effectively.

2. **User Feedback-Aware Reflection Mechanism**: The system incorporates a reflection mechanism that utilizes user feedback to adjust dialogue act planning dynamically. This mechanism operates on two levels:
   - **Information-Level Reflection**: Summarizes user feedback into higher-level user profiles, capturing user preferences and browsing history.
   - **Strategy-Level Reflection**: Analyzes past interactions to deduce reasons for recommendation failures and provides corrective experiences to improve future recommendations.

3. **Experimental Validation**: The authors conducted extensive experiments using a user simulator to demonstrate the effectiveness of MACRS in enhancing user interaction experiences and improving recommendation accuracy compared to traditional and LLM-based CRS methods.

### Results:
- MACRS outperformed existing methods in terms of success rate, average turns, and hit ratio for recommendations.
- The multi-agent approach allowed for more diverse dialogue act selection, leading to a more engaging user experience.

### Conclusion:
The proposed MACRS framework significantly improves the performance of conversational recommendation systems by effectively managing dialogue flow and leveraging user feedback. The findings suggest that integrating multi-agent systems with LLMs can enhance the capabilities of conversational recommender systems in real-world applications.

For further details, you can refer to the full paper [here](https://arxiv.org/pdf/2402.01135).
INFO:root:Done
INFO:root:[green]User: 
INFO:root:scholar-agent is thinking...
INFO:root:Done
INFO:root:[blue]scholar-agent: 
INFO:root:{"name":"search_single_paper", "arguments":{"title":"A Multi-Agent Conversational Recommender System","top_k":1}}

INFO:root:Calling function: search_single_paper ...
INFO:scholarly:Getting https://scholar.google.com/scholar?hl=en&q=A%20Multi-Agent%20Conversational%20Recommender%20System
INFO:httpx:HTTP Request: GET https://scholar.google.com/scholar?hl=en&q=A%20Multi-Agent%20Conversational%20Recommender%20System "HTTP/1.1 200 OK"
INFO:root:Done
INFO:root:--------------------
INFO:root:{'title': 'A multi-agent conversational recommender system', 'author': ['J Fang', 'S Gao', 'P Ren', 'X Chen', 'S Verberne'], 'pub_year': '2024', 'venue': 'arXiv preprint arXiv …', 'abstract': 'Due to strong capabilities in conducting fluent, multi-turn conversations with users, Large Language Models (LLMs) have the potential to further improve the performance of Conversational Recommender System (CRS). Unlike the aimless chit-chat that LLM excels at, CRS has a clear target. So it is imperative to control the dialogue flow in the LLM to successfully recommend appropriate items to the users. Furthermore, user feedback in CRS can assist the system in better modeling user preferences, which has been ignored by', 'url': 'https://arxiv.org/abs/2402.01135', 'citation': 6}
INFO:root:--------------------
INFO:root:scholar-agent is thinking...
INFO:root:Done
INFO:root:[blue]scholar-agent: 
INFO:root:The conclusion of the paper titled "A Multi-Agent Conversational Recommender System" is as follows:

In this paper, we introduced Multi-Agent Conversational Recommender System (MACRS), an LLM-only multi-agent Conversational Recommender System (CRS) that leverages several different agents to tackle the dialogue flow planning and integrate the user feedback to optimize the dialogue planning dynamically. We first propose a novel multi-agent dialogue planning module to separately model each dialogue act using LLM-based responder agents and schedule these responder agents according to the dialogue strategy and user feedback by using the planner agent. Then we propose a user feedback-aware reflection module that conducts the reflection on both information-level and strategy-level to produce high-level user preference descriptions and dialogue strategy suggestions for the subsequent dialogue. These two modules can collaborate to optimize the dialogue flow and increase the user experience and recommendation accuracy. Experiments conducted on a benchmark CRS dataset demonstrate that our proposed MACRS can boost the performance of recommendations significantly and have a better and more engaging user experience than with the existing CRS methods.
INFO:root:Done
INFO:root:[green]User: 
INFO:root:
[red]Bye!
